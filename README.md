# Cutter-Model
Модель для определения склеек кадров в фильмах

## Запуск предобученных моделей
Дать ссылки для быстрого запуска модели локально или в colab. А так же ссылки на модель и датасет с HaggingFace.

## Введение
Приветствую всех в моём разборе решения задачи нахождения склеек в фильмах! Во-первых, стоит объяснить зачем мне пришлось находить склейки. В марте 2023 года проходил хакатон по созданию AI решений по двум трекам, одним из которых звучал так: "Аудиосопровождение происходящего на экране для людей с нарушением зрения". Наше решение вы можете посмотреть <a href='https://github.com/Aleshka5/MTS_UAI_Team'>тут</a>.<br>

Как понятно из названия задача заключалась в изучении фильмов средствами AI. Одним из первых решений нашей команды было описать каждую сцену отдельно (далее подход мы улучшили, но сейчас не об этом). Так, информация не перемешивалась при смене локации и действующих лиц, ведь они порой не связаны друг с другом напрямую. После чего я занялся решением этой задачи.
![МТС борд](https://github.com/Aleshka5/Cutter-Model/assets/78702396/b61396d6-1124-4cc5-8cd9-4b8aff03b53a)

## Постановка задачи
Для начала разберёмся в природе склеек и их типах. <br>

> <b>Склейка</b> (в данном разборе) - два кадра, которые принадлежат разным локациям, сценам, ракурсам одной сцены и т.д.

То есть, если камеру выключили и переместили, а потом продолжили снимать, то мы будем искать этот момент по двум соседним кадрам. Такой подход оптимален в 95% случаев.<br>
![Склейка пример 1 rus](https://github.com/Aleshka5/Cutter-Model/assets/78702396/ecc96631-dd14-49d5-b290-65b82f0359a5)
### Классификация склеек
Тут всё просто, есть три типа склеек: 
<ol>
  <li>Явная склейка</li>
  <li>Плавное наложение разных сцен</li>
  <li>Плавное затемнение (засветление)</li>
</ol>
Последние два типа склеек как правило и будут составлять 5% ошибок, потому что в них кадры меняются плавно (не считая динамичных моментов, когда нейросеть просто ошибается внутри одной сцены).

![Пример тип 2 и тип 3 rus](https://github.com/Aleshka5/Cutter-Model/assets/78702396/1656b30f-8636-47ab-a641-136ecee50a6e)

## Предобработка данных и сбор датасета
Для начала решения задачи я решил написать не большой скрипт для удобного сбора и разметки датасета. Сам датасет я решил собирать из 4 трейлеров к фильмам, которые в сумме длятся не более 20ти минут. Всё из-за того, что трейлеры очень плотно нафаршированы склейками.

### Стандартизация размера входных данных
Для начала работы с видео мы должны перевести их в список кадров и привести к одному размеру, у меня это будет 54x128, так как при таком соотношении сторон обычно снимают фильмы и это достаточное разрешение, чтобы нейросеть работала быстро и замечала детали на видео. 
В библиотеке cv2 есть отличаня функция, которая помогает изображения людого разрашения приводить к тем, которые мы хоитм видеть.<br> Чтение кадров видео в нужном размере происходит так:
```python
cap = cv2.VideoCapture(video_path)        
success, img = cap.read()
resized = cv2.resize(img, (128, 54), interpolation=cv2.INTER_AREA)         # 1.2 
```

### Сбор данных
Так как полнометражные фильмы больше трейлеров, а весь фильм покадрово в RAM не поместится приходим к выводу, что стоит заранее позаботится о его постепенной обработке. Фильм мы будем обрабатывать кусками (батчами). Ниже представлен текст функции для разбиения видео файла на кадры генератором. Преимущество генератора в этой задаче заключается в отсутствии необходимости заново прочитывать ненужные кадры, когда мы берём следующий батч, так как генератор сохраняет своё состояние после прерывания ```yield```.
```python
def generator_particular_frames_from_video( video_path, W, H, part_size, part_count = -1):
    '''
    Генератор батчей пар кадров. Результатом является массив пар соседних кадров.
    
    param: 
    video_path - путь до видео файла, который необходимо конвертировать в кадры, 
    W (width) - ширина кадра, 
    H (height) - высота кадра, 
    part_size - размер батча кадров,
    part_count - количество батчей необходимых к прочтению. (= -1 если нужно прочесть весь видео файл)
    
    return:
    numpy_images - кадры видео файла.
    real_size - батчи считываются всегда одинакового размера, но последний батч может иметь меньший размер. real_size - возвращает действительный размер прочитанного батча.
    '''
    real_size = part_size
    cap = cv2.VideoCapture(video_path)    
    numpy_images = np.zeros((part_size, 2, H, W, 3), dtype='uint8')
    # Чтение кадра
    success, img1 = cap.read()                                                # 1.1    
    resized1 = cv2.resize(img1, (W, H), interpolation=cv2.INTER_AREA)         # 1.2    
    success, img2 = cap.read()                                                # 2.1
    i = 0
    # Условием прекращения создания батчей является либо конец файла видео, либо завершение по количеству прочитанных батчей
    while ((i != part_count) and (real_size == part_size)):        
        real_size = 0
        
        while ((real_size < part_size) and (success)):
            resized2 = cv2.resize(img2, (W, H), interpolation=cv2.INTER_AREA)            # 2.2            
            numpy_images[real_size, 0], numpy_images[real_size, 1] = resized1, resized2  # Записть кадров в массив            
            resized1 = resized2        # Смена кадра
            success, img2 = cap.read()
            real_size += 1        
            
        i += 1
        yield numpy_images[:real_size], real_size
```

### Сбор собственной базы данных
Далее я покажу как вы можете самостоятельно собирать такой же датасет на локальной машине.


### Аугментация датасета 
Как из маленького датасета сделать огромный

### Проблема чёрных экранов
описание проблемы и её решение
## Различные подходы и их точность
### Метрики

### Архитектура с двумя изображениями

### Архитектура с разностью изображений

### Заключение и сравнение с уже существующими аналогами

## Выводы
